---
title: Linux 收包与发包流程
date: 2023-12-04 18:04:00 +0800
author: 
categories: []
tags: [System Design]
pin: false
math: false
mermaid: false
img_path: /assets/images/RecieveAndSendImages
---

Linux 网络发包流程：<https://www.cnblogs.com/edisonfish/p/17637507.html>

Linux 网络收包流程： <https://www.cnblogs.com/edisonfish/p/17578159.html>




我们在跟别人网上聊天的时候，有没有想过你发送的信息是怎么传到对方的电脑上的

又或者我们在上网冲浪的时候，有没有想过 HTML 页面是怎么显示在我们的电脑屏幕上的

无论是我们跟别人聊天还是上网冲浪，其实都依靠于计算机网络这项技术

> 计算机网络是指将多台计算机通过通信设备和传输介质连接在一起，使得它们之间能够相互通信、资源共享和协同工作

而计算机之间是通过数据包来实现信息传输和信息交换的，数据包是计算机网络中传输数据的基本单位

今天咸鱼将以 Linux 为例来给大家介绍一下 **Linux 是如何实现网络接收数据包**

## 网络协议栈&网络架构

在正文开始之前，我们先来了解一下 Linux 中的网络协议模型和网络子系统

- **网络协议模型（网络协议栈）**

在 Linux 中，Linux 网络协议栈分成了五层

![image](2958925-20230724194233187-1879013908.png)

其中：

- 应用层提供 socket 接口来供用户进程访问内核空间的网络协议栈
- 传输层、网络层协议由 Linux 内核网络协议栈实现
- 链路层协议靠网卡驱动来实现
- 物理层协议由硬件网卡实现

  ![image](2958925-20230724194246044-504162171.png)

- **网络子系统（网络架构）**

网络子系统是 Linux 内核中的一部分，由多个模块和驱动程序组成，它负责管理和控制系统的网络功能以实现网络通信

通过 Linux 网络子系统（网络架构）来实现上述网络协议模型

![image](2958925-20230724194254616-1132842294.png)

其中

- System call interface：为应用程序获取内核的网络系统提供了接口，例如 socket
- Protocol agnostic interface：为和各种传输层协议的网络交互提供的一层公共接口
- Network protocals：对各种传输层协议的实现，如 TCP、UDP、IP 等
- Device agnostic interface：为各种底层网络设备抽象出的公共接口，与各种网络设备驱动连接在一起
- Device drivers：与各种网络设备交互的驱动

## 收包过程

当 Linux 接收一个数据包的时候，这个包是怎么经过 Linux 的内核从而被应用程序拿到的呢？

![image](2958925-20230724194307066-891608593.png)

- **到达网卡（NIC，Network Interface Card）**

首先数据包到达网卡之后，网卡会校验接收到的数据包中的目的 MAC 地址是不是自己的 MAC 地址，如果不是的话通常就会丢弃掉

这种只接受发送给自己的数据包（其余的扔掉）的工作模式称为非**混杂模式（Non-Promiscuous Mode）**

**混杂模式（Promiscuous Mode）**则是网卡会接收通过网络传输的所有数据包，而不仅仅是发送给它自己的数据包

非混杂模式是网卡默认的工作模式，可以尽可能的保护网络安全和减少网络负载

网卡在校验完 MAC 地址之后还会校验数据帧（Data Frame）中校验字段 FCS 来一次确保接收到的数据包是正确的

- **网卡硬件缓冲区 ——> 系统内存（ring buffer）**

当网卡接收到数据包时，它将数据包的内容存储在硬件缓冲区中，然后通过 DMA 将接收到的数据从硬件缓冲区传输到系统内存中的指定位置，这个位置通常是一个环形缓冲区（ ring buffer）

> DMA（直接内存访问，Direct Memory Access）
>
> DMA是一种数据传输技术，允许外设（如网卡、硬盘控制器、显卡等）直接访问计算机内存，而无需经过 CPU
>
> 通过 DMA 可以大大提高数据传输的效率，减轻 CPU 的负担

- **触发硬中断**

当网卡将数据包 DMA 到用于接收的环形缓冲区（rx_ring）之后，就会触发一个硬中断来告诉 CPU 数据包收到了

什么时候会触发一个硬中断，可以通过下面的参数来进行配置：

- rx-usecs：当过这么长时间过后，一个中断就会被产生
- rx-frames：当累计接收到这么多个数据帧后，一个中断就会被产生

上面的参数配置可以通过下面的命令来查看

```bash
# 以 CentOS 7 为例
ethtool -c <网卡名称>
```

> 当 ring buffer 满了之后，新来的数据包将给丢弃
>
> ifconfig 查看网卡的时候，可以里面有个 overruns，表示因为环形队列满而被丢弃的包

CPU 收到硬中断之后就会停止手中的活，保存上下文，然后去调用网卡驱动注册的硬中断处理函数

为数据包分配 `skb_buff` ，并将接收到的数据拷贝到 `skb_buff` 缓冲区中

> 当一个数据包经过了网卡引起中断之后，每一个包都会在内存中分配一块区域，称为 `sk_buff` (套接字缓存，socket buffer )
>
> `sk_buff` 是 Linux 网络的一个核心数据结构

- **触发软中断**

网卡的硬中断处理函数处理完之后驱动先 disable 硬中断，然后 enable 软中断

> ps：待 ring buffer 中的所有数据包被处理完成后，enable 网卡的硬中断，这样下次网卡再收到数据的时候就会通知 CPU

内核负责软中断进程 `ksoftirqd` 发现有软中断请求到来，进行下面的一些操作

```bash
# 查看软中断进程
[root@localhost ~]# ps -ef | grep ksoftirqd
```

调用 `net_rx_action` 函数

它会通过 `poll` 函数去 `rx_ring` 中拿数据帧，获取的时候顺便把 `rx_ring` 上的数据给删除

```c++
static void net_rx_action(struct softirq_action *h)
{
    struct softnet_data *sd = &__get_cpu_var(softnet_data);
    unsigned long time_limit = jiffies + 2;
    int budget = netdev_budget;
    void *have;

    local_irq_disable();

    while (!list_empty(&sd->poll_list)) {
        ......
        n = list_first_entry(&sd->poll_list, struct napi_struct, poll_list);

        work = 0;
        if (test_bit(NAPI_STATE_SCHED, &n->state)) {
            work = n->poll(n, weight);
            trace_napi_poll(n);
        }

        budget -= work;
    }
}
```

除此之外，`poll` 函数会把 `ring buffer` 中的数据包转换成内核网络模块能够识别的 skb 格式（即 `socket kernel buffer`）

> socket kernel buffer （skb） 是 Linux 内核网络栈处理网络包（packets）所使用的 buffer，它的类型是 sk_buffer

3、最后进入 `netif _receive_skb` 处理流程，它是数据链路层接收数据帧的最后一关

根据注册在全局数组 `ptype_all` 和 `ptype_base` 里的网络层数据帧类型去调用第三层协议的接收函数处理

> 例如对于 ip 包来讲，就会进入到 `ip_rcv`；如果是 arp 包的话，会进入到 `arp_rcv`

- **到达网络层（以 IP 协议为例）**

IP 层的入口函数在 `ip_rcv` 函数，调用 `ip_rcv` 函数进入三层协议栈

首先会对数据包进行各种检查（检查 IP Header），然后调用 `netfilter` 中的钩子函数： `NF_INET_PRE_ROUTING`

> netfilter： 是 Linux 内核中进行数据包过滤，连接跟踪（Connect Track），网络地址转换（NAT）等功能的主要实现框架
>
> 该框架在网络协议栈处理数据包的关键流程中定义了一系列钩子点（Hook 点），并在这些钩子点中注册一系列函数对数据包进行处理
>
> 这些注册在钩子点的函数即为设置在网络协议栈内的数据包通行策略，也就意味着，这些函数可以决定内核是接受还是丢弃某个数据包

`NF_INET_PRE_ROUTING` 会根据预设的规则对数据包进行判断并根据判断结果做相关的处理（修改或者丢弃数据包）

处理完成后，数据包交由 `ip_rcv_finish` 处理，该函数根据路由判决结果，决定数据包是交由本机上层应用处理，还是需要进行转发

如果是交由本机处理，则会交由 `ip_local_deliver` 本地上交流程；如果需要转发，则交由 `ip_forward` 函数走转发流程

- **到达传输层（以 TCP 为例）**

传输层 TCP 处理入口在 `tcp_v4_rcv` 函数，首先检查数据包的 TCP 头部等信息，确保数据包的完整性和正确性

然后去查找该数据包对应的已经打开的 socket ，如果找不到匹配的 socket，表示该数据包不属于任何一个已建立的连接，因此该数据包会被丢弃

如果找到了匹配的 socket，TCP 会进一步检查该 socket 和连接的状态，如果状态正常，TCP 会将数据包从内核传输到用户空间，放入 socket 的接收缓冲区（socket receive buffer）

- **应用层获取数据**

当数据包到达操作系统内核的传输层时，应用程序可以从套接字的接收缓冲区（socket receive buffer）中读取数据包

一般有两种方式读取数据，一种是 `recvfrom` 函数阻塞在那里等着数据来，这种情况下当 socket 收到通知后，`recvfrom` 就会被唤醒，然后读取接收队列的数据

另一种是通过 epoll 或者 select 监听相应的 socket，当收到通知后，再调用 `recvfrom` 函数去读取接收队列的数据

## 收包总结

网络模块可以说是 Linux 内核中最复杂的模块了

看起来一个简简单单的收包过程就涉及到许多内核组件之间的交互，如网卡驱动、协议栈，内核ksoftirqd 线程等

咸鱼原本打算把收包和发包的流程都写上的，但是光是写收包流程就就要了我半条命了，等下次有机会把发包的流程也写一下

总结一下 Linux 网络收包流程：

- 数据到达网卡之后，网卡通过 DMA 将数据放到内存分配好的一块 `ring buffer` 中，然后触发硬中断
- CPU 收到硬中断之后简单的处理了一下（分配 `skb_buffer`），然后触发软中断
- 软中断进程 `ksoftirqd` 执行一系列操作（例如把数据帧从 `ring ruffer`上取下来）然后将数据送到三层协议栈中
- 在三层协议栈中数据被进一步处理发送到四层协议栈
- 在四层协议栈中，数据会从内核拷贝到用户空间，供应用程序读取
- 最后被处在应用层的应用程序去读取



---



简单回顾一下：

- 数据到达网卡之后，网卡通过 DMA 将数据放到内存分配好的一块 `ring buffer` 中，然后触发硬中断
- CPU 收到硬中断之后简单的处理了一下（分配 `skb_buffer`），然后触发软中断
- 软中断进程 `ksoftirqd` 执行一系列操作（例如把数据帧从 `ring ruffer`上取下来）然后将数据送到三层协议栈中
- 在三层协议栈中数据被进一步处理发送到四层协议栈
- 在四层协议栈中，数据会从内核拷贝到用户空间，供应用程序读取
- 最后被处在应用层的应用程序去读取

当 Linux 要发送一个数据包的时候，这个包是怎么从应用程序再到 Linux 的内核最后由网卡发送出去的呢？

那么今天咸鱼将会为大家介绍 **Linux 是如何实现网络发送数据包**

## 发包流程

假设我们的网卡已经启动好（分配和初始化 RingBuffer） 且 server 和 client 已经建立好 socket

这里需要注意的是，网卡在启动过程中申请分配的 RingBuffer 是有两个：

- `igb_tx_buffer` 数组：这个数组是内核使用的，**用于存储要发送的数据包描述信息**，通过 `vzalloc`申请的
- `e1000_adv_tx_desc` 数组：这个数组是网卡硬件使用的，**用于存储要发送的数据包**，网卡硬件可以通过 DMA 直接访问这块内存，通过 `dma_alloc_coherent`分配

> ```
> igb_tx_buffer` 数组中的每个元素都有一个指针指向 `e1000_adv_tx_desc
> ```
>
> 这样内核就可以把要发送的数据填充到 `e1000_adv_tx_desc` 数组上
>
> 然后网卡硬件会直接从 `e1000_adv_tx_desc` 数组中读取实际数据，并将数据发送到网络上

![image](2958925-20230817143041842-24587835.png)

### 拷贝到内核

- **socket 系统调用将数据拷贝到内核**

应用程序首先通过 socket 提供的接口实现系统调用

我们在用户态使用的 `send` 函数和 `sendto` 函数其实都是 `sendto` 系统调用实现的

`send/sendto`函数 只是为了用户方便，封装出来的一个更易于调用的方式而已

```c
/* sendto 系统调用 省略了一些代码 */
SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
		unsigned int, flags, struct sockaddr __user *, addr,
		int, addr_len)
{
	...
	sock = sockfd_lookup_light(fd, &err, &fput_needed);
	...
	err = sock_sendmsg(sock, &msg, len);
	...	
}
```

在 `sendto` 系统调用内部，首先 `sockfd_lookup_light` 函数会查找与给定文件描述符（fd）关联的 socket

接着调用 `sock_sendmsg` 函数（`sock_sendmsg ==> __sock_sendmsg ==> __sock_sendmsg_nosec`）

其中 `sock->ops->sendmsg` 函数实际执行的是 `inet_sendmsg` 协议栈函数

```c
/*
__sock_sendmsg_nosec 函数

iocb：指向与 I/O 操作相关的结构体 kiocb
sock: 指向要执行发送操作的套接字结构体
msg: 指向存储要发送数据的消息头结构体 msghdr
size: 要发送的数据大小

*/
static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock,
				       struct msghdr *msg, size_t size)
{
	...
	return sock->ops->sendmsg(iocb, sock, msg, size);
}
```

这时候内核会去找 socket 上对应的具体协议发送函数

以 TCP 为例，具体协议发送函数为 `tcp_sendmsg`

![image](2958925-20230817143051058-1851777111.png)

`tcp_sendmsg` 会去申请一个内核态内存 `skb（sk_buff）` ，然后挂到发送队列上（发送队列是由 skb 组成的一个链表）

![image](2958925-20230817143106124-469053530.png)

接着把用户待发送的数据拷贝到 skb 中，拷贝之后会触发【发送】操作

> 这里说的发送是指在当前上下文中，待发送数据从 socket 层发送到传输层

**需要注意的是**，这时候不一定开始真正发送，因为还要进行一些条件判断（比如说发送队列中的数据已经超过了窗口大小的一半）

只有满足了条件才能够发送，如果没有满足条件这次系统调用就可能直接返回了

### 网络协议栈处理

- **传输层处理**

接着数据来到了传输层

传输层主要看 `tcp_write_xmit` 函数，这个函数处理了传输层的拥塞控制、滑动窗口相关的工作

> 该函数会根据发送窗口和最大段大小等因素计算出本次发送的数据大小，然后将数据封装成 TCP 段并发送出去

如果满足窗口要求，设置 TCP 头然后将数据传到更低的网络层进行处理

在传输层中，内核主要做了两件事：

- 复制一份数据（skb）

为什么要复制一份出来呢？因为网卡发送完成之后，skb 会被释放掉，**但 TCP 协议是支持丢失重传的**

所以在收到对方的 ACK 之前必须要备份一个 skb 去为重传做准备

> 实际上一开始发送的是 skb 的拷贝版，收到了对方的 ACK 之后系统才会把真正的 skb 删除掉

- 封装 TCP 头

系统会根据实际情况添加 TCP 头封装成 TCP 段

这里需要知道的是：**每个 skb 内部包含了网络协议中的所有头部信息**，例如 MAC 头、IP 头、TCP/UDP 头等

在设置这些头部时，内核会通过调整指针的位置来填充相应的字段，而不是频繁申请和拷贝内存

![image](2958925-20230817143135784-806131086.png)

> 比如说在设置 TCP 头的时候，只是把指针指向 skb 的合适位置。后面再设置 IP 头的时候，在把指针挪一挪就行

这种方式利用了 skb 数据结构的链表特性可以避免内存分配和数据拷贝所带来的性能开销，从而提高数据传输的效率

- **网络层处理**

数据离开了传输层之后，就来到了网络层

网络层主要做下面的事情：

- 路由项查找：

根据目标 IP 地址查找路由表，确定数据包的下一跳（`ip_queue_xmit` 函数）

- IP 头设置：

根据路由表查找的结果，设置 IP 头中的源和目标 IP 地址、TTL（生存时间）、IP 协议等字段

- netfilter 过滤：

netfilter 是 Linux 内核中的一个框架，用于实现数据包的过滤和修改

在网络层，netfilter 可以用于对数据包进行过滤、NAT（网络地址转换）等操作

- skb 切分：

如果数据包的大小超过了 MTU（最大传输单元），需要将数据包进行切分成多个片段，以适应网络传输，每个片段会被封装成单独的 skb

- **数据链路层处理**

当数据来到了数据链路层之后，会有两个子系统协同工作，确保数据包在发送和接收过程中能够正确地对数据进行封装、解析和传输

- 邻居子系统

管理和维护主机或路由器与其它设备之间的邻居关系

邻居子系统里会发送 arp 请求找邻居，然后把邻居信息存在邻居缓存表里，用于存储目标主机的 MAC 地址

当需要发送数据包到某个目标主机时，数据链路层会首先查询邻居缓存表，以获取目标主机的 MAC 地址，从而正确地封装数据包（封装 MAC 头）

- 网络设备子系统

网络设备子系统负责处理与物理网络接口相关的操作，包括数据包的封装和发送，以及从物理接口接收数据包并进行解析

> 网络设备子系统不但处理数据包的格式转换，如在以太网中添加帧头和帧尾，以及从帧中提取数据
>
> 还负责处理硬件相关的操作，如发送和接收数据包的时钟同步、物理层错误检测等

- 到达网卡发送队列

接着网络设备子系统会选择一个合适的网卡发送队列并把 skb 添加到队列中（绕过软中断处理程序）

然后，内核会调用网卡驱动的入口函数 `dev_hard_start_xmit` 来触发数据包的发送

> 在一些情况下，邻居子系统还会将 skb 数据包添加到软中断队列（softnet_data）上，并触发软中断（NET_TX_SOFTIRQ）
>
> 这个过程是为了将 skb 数据包交给软中断处理程序进行进一步处理和发送。软中断处理程序会负责实际的数据包发送

**这就是为什么一般服务器上查看 `/proc/softirqs`，一般 NET_RX 都要比 NET_TX 大的多的原因之一**

即对于收包来说，都是要经过 NET_RX 软中断；而对于发包来说，只有某些情况下才触发 NET_TX 软中断

### 网卡驱动发送

驱动程序从发送队列中读取 skb 的描述信息，将其挂到 RingBuffer 上（前面提到的`igb_tx_buffer` 数组）

接着将 skb 的描述信息映射到网卡可访问的内存 DMA 区域中（前面提到的`e1000_adv_tx_desc` 数组）

网卡会直接从 `e1000_adv_tx_desc` 数组中根据描述信息读取实际数据并将数据发送到网络。这样就完成了数据包的发送过程

### 收尾工作

当数据发送完成后，网卡设备会触发一个硬件中断（**NET_RX_SOFTIRQ**），这个硬中断通常称为“发送完成中断”或者“发送队列清理中断”

这个硬中断的主要作用是执行发送完成的清理工作，包括释放之前为数据包分配的内存，即释放 skb 内存和 RingBuffer 内存

最后，当收到这个 TCP 报文的 ACK 应答时，传输层就会释放原始的 skb（前面有讲到发送的其实是 skb 的拷贝版）

> 可以看到，当数据发送完成以后，通过硬中断的方式来通知驱动发送完毕，而这个中断类型是 `NET_RX_SOFTIRQ`
>
> 前面我们讲到过网卡收到一个网络包的时候，会触发 `NET_RX_SOFTIRQ`中断去告诉 CPU 有数据要处理
>
> 也就是说，无论是网卡接收一个网络包还是发送网络包结束之后，触发的都是 `NET_RX_SOFTIRQ`

## 发包总结

最后总结一下在 Linux 系统中发送网络数据包的流程：

![image](2958925-20230817143149658-816269766.png)

- 应用程序通过 socket 提供的接口进行系统调用，将数据从用户态拷贝到内核态的 socket 缓冲区中
- 网络协议栈从 socket 缓冲区中拿取数据，并按照 TCP/IP 协议栈从上到下逐层处理
  - 传输层处理：以 TCP 为例，在传输层中会复制一份数据（为了丢失重传），然后为数据封装 TCP 头
  - 网络层处理：选取路由（确认下一跳的 IP）、填充 IP 头、netfilter 过滤、对超过 MTU 大小的数据包进行分片等操作
  - 邻居子系统和网络设备子系统处理：在这里数据会被进一步处理和封装，然后被添加到网卡的发送队列中
- 驱动程序从发送队列中读取 skb 的描述信息然后挂在 RingBuffer 上，接着将 skb 的描述信息映射到网卡可访问的内存 DMA 区域中
- 网卡将数据发送到网络
- 当数据发送完成后触发硬中断，释放 skb 内存和 RingBuffer 内存